{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agent Memory: Building Memory-Enabled Investment Agents with LangGraph\n\nIn this notebook, we'll explore **agent memory systems** - the ability for AI agents to remember information across interactions. We'll implement all five memory types from the **CoALA (Cognitive Architectures for Language Agents)** framework while building a Stone Ridge Investment Advisory Assistant.\n\n**Learning Objectives:**\n- Understand the 5 memory types from the CoALA framework\n- Implement short-term memory with checkpointers and thread_id\n- Build long-term memory with InMemoryStore and namespaces\n- Use semantic memory for meaning-based retrieval\n- Apply episodic memory for few-shot learning from past experiences\n- Create procedural memory for self-improving agents\n- Combine all memory types into a unified investment advisory agent\n\n## Table of Contents:\n\n- **Breakout Room #1:** Memory Foundations\n  - Task 1: Dependencies\n  - Task 2: Understanding Agent Memory (CoALA Framework)\n  - Task 3: Short-Term Memory (MemorySaver, thread_id)\n  - Task 4: Long-Term Memory (InMemoryStore, namespaces)\n  - Task 5: Message Trimming & Context Management\n  - Question #1 & Question #2\n  - ğŸ—ï¸ Activity #1: Store & Retrieve User Investment Profile\n\n- **Breakout Room #2:** Advanced Memory & Integration\n  - Task 6: Semantic Memory (Embeddings + Search)\n  - Task 7: Building Semantic Investment Knowledge Base\n  - Task 8: Episodic Memory (Few-Shot Learning)\n  - Task 9: Procedural Memory (Self-Improving Agent)\n  - Task 10: Unified Investment Memory Agent\n  - Question #3 & Question #4\n  - ğŸ—ï¸ Activity #2: Investment Memory Dashboard"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ¤ Breakout Room #1\n",
    "## Memory Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "1. **API Keys** for:\n",
    "   - OpenAI (for GPT-4o-mini and embeddings)\n",
    "   - LangSmith (optional, for tracing)\n",
    "\n",
    "2. **Dependencies installed** via `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: LangSmith for tracing\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE9 - Agent Memory - Investment - {uuid4().hex[0:8]}\"\nos.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key (press Enter to skip): \") or \"\"\n\nif not os.environ[\"LANGCHAIN_API_KEY\"]:\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n    print(\"LangSmith tracing disabled\")\nelse:\n    print(f\"LangSmith tracing enabled. Project: {os.environ['LANGCHAIN_PROJECT']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory systems ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Say 'Memory systems ready!' in exactly those words.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Task 2: Understanding Agent Memory (CoALA Framework)\n\nThe **CoALA (Cognitive Architectures for Language Agents)** framework identifies 5 types of memory that agents can use:\n\n| Memory Type | Human Analogy | AI Implementation | Investment Example |\n|-------------|---------------|-------------------|------------------|\n| **Short-term** | What someone just said | Conversation history within a thread | Current investment consultation conversation |\n| **Long-term** | Remembering a friend's birthday | User preferences stored across sessions | User's risk tolerance, portfolio size, investment goals |\n| **Semantic** | Knowing Paris is in France | Facts retrieved by meaning | Investment knowledge retrieval |\n| **Episodic** | Remembering your first day at work | Learning from past experiences | Past successful advisory patterns |\n| **Procedural** | Knowing how to ride a bike | Self-improving instructions | Learned communication and advisory preferences |\n\n### Memory Architecture Overview\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                LangGraph Investment Advisory Agent               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚  Short-term  â”‚  â”‚  Long-term   â”‚  â”‚   Semantic   â”‚           â”‚\nâ”‚  â”‚    Memory    â”‚  â”‚    Memory    â”‚  â”‚    Memory    â”‚           â”‚\nâ”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚           â”‚\nâ”‚  â”‚ Checkpointer â”‚  â”‚    Store     â”‚  â”‚Store+Embed   â”‚           â”‚\nâ”‚  â”‚ + thread_id  â”‚  â”‚ + namespace  â”‚  â”‚  + search()  â”‚           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\nâ”‚  â”‚   Episodic   â”‚  â”‚  Procedural  â”‚                             â”‚\nâ”‚  â”‚    Memory    â”‚  â”‚    Memory    â”‚                             â”‚\nâ”‚  â”‚              â”‚  â”‚              â”‚                             â”‚\nâ”‚  â”‚  Few-shot    â”‚  â”‚Self-modifyingâ”‚                             â”‚\nâ”‚  â”‚  examples    â”‚  â”‚   prompts    â”‚                             â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Key LangGraph Components\n\n| Component | Memory Type | Scope |\n|-----------|-------------|-------|\n| `MemorySaver` (Checkpointer) | Short-term | Within a single thread |\n| `InMemoryStore` | Long-term, Semantic, Episodic, Procedural | Across all threads |\n| `thread_id` | Short-term | Identifies unique conversations |\n| Namespaces | All store-based | Organizes memories by user/purpose |\n\n**Documentation:**\n- [CoALA Paper](https://arxiv.org/abs/2309.02427)\n- [LangGraph Memory Concepts](https://langchain-ai.github.io/langgraph/concepts/memory/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Short-Term Memory (MemorySaver, thread_id)\n",
    "\n",
    "**Short-term memory** maintains context within a single conversation thread. Think of it like your working memory during a phone call - you remember what was said earlier, but once the call ends, those details fade.\n",
    "\n",
    "In LangGraph, short-term memory is implemented through:\n",
    "- **Checkpointer**: Saves the graph state at each step\n",
    "- **thread_id**: Uniquely identifies each conversation\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Thread 1: \"Hi, I'm Alice\"          Thread 2: \"What's my name?\"\n",
    "     â”‚                                   â”‚\n",
    "     â–¼                                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Checkpointer â”‚                   â”‚ Checkpointer â”‚\n",
    "â”‚  thread_1    â”‚                   â”‚  thread_2    â”‚\n",
    "â”‚              â”‚                   â”‚              â”‚\n",
    "â”‚ [\"Hi Alice\"] â”‚                   â”‚ [empty]      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚                                   â”‚\n",
    "     â–¼                                   â–¼\n",
    "\"Hi Alice!\"                        \"I don't know your name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n\n# Define the state schema for our graph\n# The `add_messages` annotation tells LangGraph how to update the messages list\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\n# Define our investment chatbot node\ndef investment_chatbot(state: State):\n    \"\"\"Process the conversation and generate an investment-focused response.\"\"\"\n    system_prompt = SystemMessage(content=\"\"\"You are a friendly Investment Advisory Assistant. \nHelp users with questions about Stone Ridge's investment philosophy, market outlook, \nportfolio strategy, and risk management.\nBe supportive and remember details the user shares about themselves.\"\"\")\n    \n    messages = [system_prompt] + state[\"messages\"]\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"chatbot\", investment_chatbot)\nbuilder.add_edge(START, \"chatbot\")\nbuilder.add_edge(\"chatbot\", END)\n\n# Compile with a checkpointer for short-term memory\ncheckpointer = MemorySaver()\ninvestment_graph = builder.compile(checkpointer=checkpointer)\n\nprint(\"Investment chatbot compiled with short-term memory (checkpointing)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test short-term memory within a thread\nconfig = {\"configurable\": {\"thread_id\": \"investment_thread_1\"}}\n\n# First message - introduce ourselves\nresponse = investment_graph.invoke(\n    {\"messages\": [HumanMessage(content=\"Hi! My name is Alex and I want to understand Stone Ridge's investment approach.\")]},\n    config\n)\nprint(\"User: Hi! My name is Alex and I want to understand Stone Ridge's investment approach.\")\nprint(f\"Assistant: {response['messages'][-1].content}\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Second message - test if it remembers (same thread)\nresponse = investment_graph.invoke(\n    {\"messages\": [HumanMessage(content=\"What's my name and what am I interested in learning about?\")]},\n    config  # Same config = same thread_id\n)\nprint(\"User: What's my name and what am I interested in learning about?\")\nprint(f\"Assistant: {response['messages'][-1].content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# New thread - it won't remember Alex!\ndifferent_config = {\"configurable\": {\"thread_id\": \"investment_thread_2\"}}\n\nresponse = investment_graph.invoke(\n    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n    different_config  # Different thread_id = no memory of Alex\n)\nprint(\"User (NEW thread): What's my name?\")\nprint(f\"Assistant: {response['messages'][-1].content}\")\nprint()\nprint(\"Notice: The agent doesn't know our name because this is a new thread!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect the state of thread 1\nstate = investment_graph.get_state(config)\nprint(f\"Thread 1 has {len(state.values['messages'])} messages:\")\nfor msg in state.values['messages']:\n    role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n    content = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n    print(f\"  {role}: {content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Long-Term Memory (InMemoryStore, namespaces)\n",
    "\n",
    "**Long-term memory** stores information across different conversation threads. This is like remembering that your friend prefers tea over coffee - you remember it every time you meet them, regardless of what you're currently discussing.\n",
    "\n",
    "In LangGraph, long-term memory uses:\n",
    "- **Store**: A persistent key-value store\n",
    "- **Namespaces**: Organize memories by user, application, or context\n",
    "\n",
    "### Key Difference from Short-Term Memory\n",
    "\n",
    "| Short-Term (Checkpointer) | Long-Term (Store) |\n",
    "|---------------------------|-------------------|\n",
    "| Scoped to a single thread | Shared across all threads |\n",
    "| Automatic (messages) | Explicit (you decide what to store) |\n",
    "| Conversation history | User preferences, facts, etc. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.store.memory import InMemoryStore\n\n# Create a store for long-term memory\nstore = InMemoryStore()\n\n# Namespaces organize memories - typically by user_id and category\nuser_id = \"user_alex\"\nprofile_namespace = (user_id, \"profile\")\npreferences_namespace = (user_id, \"preferences\")\n\n# Store Alex's investment profile\nstore.put(profile_namespace, \"name\", {\"value\": \"Alex\"})\nstore.put(profile_namespace, \"goals\", {\"primary\": \"long-term growth\", \"secondary\": \"income generation\"})\nstore.put(profile_namespace, \"constraints\", {\"risk_tolerance\": \"moderate\", \"restrictions\": [\"no tobacco stocks\"], \"esg_preference\": True})\nstore.put(profile_namespace, \"portfolio\", {\"size\": \"$500K\", \"horizon\": \"20 years\", \"current_allocation\": \"60/40 stocks/bonds\"})\n\n# Store Alex's preferences\nstore.put(preferences_namespace, \"communication\", {\"style\": \"data-driven\", \"detail_level\": \"comprehensive\"})\nstore.put(preferences_namespace, \"reporting\", {\"frequency\": \"quarterly\", \"preferred_metrics\": [\"CAGR\", \"Sharpe ratio\", \"max drawdown\"]})\n\nprint(\"Stored Alex's profile and preferences in long-term memory\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve specific memories\nname = store.get(profile_namespace, \"name\")\nprint(f\"Name: {name.value}\")\n\ngoals = store.get(profile_namespace, \"goals\")\nprint(f\"Goals: {goals.value}\")\n\n# List all memories in a namespace\nprint(\"\\nAll profile items:\")\nfor item in store.search(profile_namespace):\n    print(f\"  {item.key}: {item.value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.store.base import BaseStore\nfrom langchain_core.runnables import RunnableConfig\n\n# Define state with user_id for personalization\nclass PersonalizedState(TypedDict):\n    messages: Annotated[list, add_messages]\n    user_id: str\n\n\ndef personalized_investment_chatbot(state: PersonalizedState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"An investment chatbot that uses long-term memory for personalization.\"\"\"\n    user_id = state[\"user_id\"]\n    profile_namespace = (user_id, \"profile\")\n    preferences_namespace = (user_id, \"preferences\")\n    \n    # Retrieve user profile from long-term memory\n    profile_items = list(store.search(profile_namespace))\n    pref_items = list(store.search(preferences_namespace))\n    \n    # Build context from profile\n    profile_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in profile_items])\n    pref_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in pref_items])\n    \n    system_msg = f\"\"\"You are an Investment Advisory Assistant. You know the following about this user:\n\nPROFILE:\n{profile_text if profile_text else 'No profile stored.'}\n\nPREFERENCES:\n{pref_text if pref_text else 'No preferences stored.'}\n\nUse this information to personalize your responses. Be supportive and helpful.\"\"\"\n    \n    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\n# Build the personalized graph\nbuilder2 = StateGraph(PersonalizedState)\nbuilder2.add_node(\"chatbot\", personalized_investment_chatbot)\nbuilder2.add_edge(START, \"chatbot\")\nbuilder2.add_edge(\"chatbot\", END)\n\n# Compile with BOTH checkpointer (short-term) AND store (long-term)\npersonalized_graph = builder2.compile(\n    checkpointer=MemorySaver(),\n    store=store\n)\n\nprint(\"Personalized graph compiled with both short-term and long-term memory\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the personalized chatbot - it knows Alex's profile!\nconfig = {\"configurable\": {\"thread_id\": \"personalized_thread_1\"}}\n\nresponse = personalized_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What investment strategy would you recommend for me?\")],\n        \"user_id\": \"user_alex\"\n    },\n    config\n)\n\nprint(\"User: What investment strategy would you recommend for me?\")\nprint(f\"Assistant: {response['messages'][-1].content}\")\nprint()\nprint(\"Notice: The agent knows about Alex's risk tolerance and portfolio without him mentioning it!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Even in a NEW thread, it still knows Alex's profile\n# because long-term memory is cross-thread!\n\nnew_config = {\"configurable\": {\"thread_id\": \"personalized_thread_2\"}}\n\nresponse = personalized_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"Are there any risks I should be aware of given my portfolio?\")],\n        \"user_id\": \"user_alex\"\n    },\n    new_config\n)\n\nprint(\"User (NEW thread): Are there any risks I should be aware of given my portfolio?\")\nprint(f\"Assistant: {response['messages'][-1].content}\")\nprint()\nprint(\"Notice: Even in a new thread, the agent knows Alex's portfolio and constraints!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Message Trimming & Context Management\n",
    "\n",
    "Long conversations can exceed the LLM's context window. LangGraph provides utilities to manage message history:\n",
    "\n",
    "- **`trim_messages`**: Keeps only recent messages up to a token limit\n",
    "- **Summarization**: Compress older messages into summaries\n",
    "\n",
    "### Why Trim Even with 128K Context?\n",
    "\n",
    "Even with large context windows:\n",
    "1. **Cost**: More tokens = higher API costs\n",
    "2. **Latency**: Larger contexts take longer to process\n",
    "3. **Quality**: Models can struggle with \"lost in the middle\" - important info buried in long contexts\n",
    "4. **Relevance**: Old messages may not be relevant to current query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.messages import trim_messages\n\n# Create a trimmer that keeps only recent messages\ntrimmer = trim_messages(\n    max_tokens=500,  # Keep messages up to this token count\n    strategy=\"last\",  # Keep the most recent messages\n    token_counter=llm,  # Use the LLM to count tokens\n    include_system=True,  # Always keep system messages\n    allow_partial=False,  # Don't cut messages in half\n)\n\n# Example: Create a long conversation\nlong_conversation = [\n    SystemMessage(content=\"You are an investment advisory assistant.\"),\n    HumanMessage(content=\"I want to improve my portfolio returns.\"),\n    AIMessage(content=\"Great goal! Let's start with your current allocation. What does your portfolio look like?\"),\n    HumanMessage(content=\"I have about 60% stocks and 40% bonds.\"),\n    AIMessage(content=\"That's a balanced allocation. For higher returns, you might consider increasing equity exposure or adding alternative investments.\"),\n    HumanMessage(content=\"What about international diversification?\"),\n    AIMessage(content=\"International exposure can reduce risk through diversification. Consider allocating 20-30% to international developed and emerging markets.\"),\n    HumanMessage(content=\"And alternative investments?\"),\n    AIMessage(content=\"Alternatives like reinsurance, real estate, and commodities can provide uncorrelated returns and enhance portfolio efficiency.\"),\n    HumanMessage(content=\"What's the most important change I should make first?\"),\n]\n\n# Trim to fit context window\ntrimmed = trimmer.invoke(long_conversation)\nprint(f\"Original: {len(long_conversation)} messages\")\nprint(f\"Trimmed: {len(trimmed)} messages\")\nprint(\"\\nTrimmed conversation:\")\nfor msg in trimmed:\n    role = type(msg).__name__.replace(\"Message\", \"\")\n    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n    print(f\"  {role}: {content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summarization approach for longer conversations\n\ndef summarize_conversation(messages: list, max_messages: int = 6) -> list:\n    \"\"\"Summarize older messages to manage context length.\"\"\"\n    if len(messages) <= max_messages:\n        return messages\n    \n    # Keep the system message and last few messages\n    system_msg = messages[0] if isinstance(messages[0], SystemMessage) else None\n    content_messages = messages[1:] if system_msg else messages\n    \n    if len(content_messages) <= max_messages:\n        return messages\n    \n    old_messages = content_messages[:-max_messages+1]\n    recent_messages = content_messages[-max_messages+1:]\n    \n    # Summarize old messages\n    summary_prompt = f\"\"\"Summarize this conversation in 2-3 sentences, \ncapturing key investment topics discussed and any important user information:\n\n{chr(10).join([f'{type(m).__name__}: {m.content[:200]}' for m in old_messages])}\"\"\"\n    \n    summary = llm.invoke(summary_prompt)\n    \n    # Return: system + summary + recent messages\n    result = []\n    if system_msg:\n        result.append(system_msg)\n    result.append(SystemMessage(content=f\"[Previous conversation summary: {summary.content}]\"))\n    result.extend(recent_messages)\n    \n    return result\n\n\n# Test summarization\nsummarized = summarize_conversation(long_conversation, max_messages=4)\nprint(f\"Summarized: {len(summarized)} messages\")\nprint(\"\\nSummarized conversation:\")\nfor msg in summarized:\n    role = type(msg).__name__.replace(\"Message\", \"\")\n    content = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n    print(f\"  {role}: {content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## â“ Question #1:\n\nWhat are the trade-offs between **short-term memory** (checkpointer) vs **long-term memory** (store)? When should investment data move from short-term to long-term? Consider:\n- What information should persist across sessions?\n- What are the compliance implications?\n- How would you decide what to promote from short-term to long-term?\n\n##### Answer:\n*Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## â“ Question #2:\n\nWhy use message trimming with a 128K context window when the Stone Ridge investor letter is relatively small? What should **always** be preserved when trimming an investment consultation?\n\nConsider:\n- The \"lost in the middle\" phenomenon\n- Cost and latency implications\n- What user information is critical for safety (risk tolerance, constraints, etc.)\n\n##### Answer:\n*Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ—ï¸ Activity #1: Store & Retrieve User Investment Profile\n\nBuild a complete investment profile system that:\n1. Defines an investment profile schema (name, risk tolerance, portfolio size, investment horizon, restrictions, goals)\n2. Creates functions to store and retrieve profile data\n3. Builds a personalized investment agent that uses the profile\n4. Tests that different users get different advice\n\n### Requirements:\n- Define at least 5 profile attributes\n- Support multiple users with different profiles\n- Agent should reference profile data in responses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### YOUR CODE HERE ###\n\n# Step 1: Define an investment profile schema\n# Example attributes: name, risk_tolerance, portfolio_size, investment_horizon, restrictions, goals, preferred_asset_classes\n\n\n# Step 2: Create helper functions to store and retrieve profiles\ndef store_investment_profile(store, user_id: str, profile: dict):\n    \"\"\"Store a user's investment profile.\"\"\"\n    pass\n\n\ndef get_investment_profile(store, user_id: str) -> dict:\n    \"\"\"Retrieve a user's investment profile.\"\"\"\n    pass\n\n\n# Step 3: Create two different user profiles\n\n\n# Step 4: Build a personalized agent that uses profiles\n\n\n# Step 5: Test with different users - they should get different advice\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ¤ Breakout Room #2\n",
    "## Advanced Memory & Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Task 6: Semantic Memory (Embeddings + Search)\n\n**Semantic memory** stores facts and retrieves them based on *meaning* rather than exact matches. This is like how you might remember \"that fund with the great risk-adjusted returns\" even if you can't remember its exact name.\n\nIn LangGraph, semantic memory uses:\n- **Store with embeddings**: Converts text to vectors for similarity search\n- **`store.search()`**: Finds relevant memories by semantic similarity\n\n### How It Works\n\n```\nUser asks: \"What helps with portfolio diversification?\"\n         â†“\nQuery embedded â†’ [0.2, 0.8, 0.1, ...]\n         â†“\nCompare with stored investment facts:\n  - \"Uncorrelated assets reduce portfolio risk\" â†’ 0.92 similarity âœ“\n  - \"Rebalancing maintains target allocations\" â†’ 0.35 similarity\n         â†“\nReturn: \"Uncorrelated assets reduce portfolio risk\"\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import OpenAIEmbeddings\n\n# Create embeddings model\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Create a store with semantic search enabled\nsemantic_store = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,  # Dimension of text-embedding-3-small\n    }\n)\n\nprint(\"Semantic memory store created with embedding support\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store various investment facts as semantic memories\nnamespace = (\"investment\", \"facts\")\n\ninvestment_facts = [\n    (\"fact_1\", {\"text\": \"Diversification across uncorrelated assets can reduce portfolio risk without sacrificing returns\"}),\n    (\"fact_2\", {\"text\": \"Stone Ridge focuses on alternative risk premiums including reinsurance and longevity risk\"}),\n    (\"fact_3\", {\"text\": \"Tail risk hedging provides insurance against extreme market downturns\"}),\n    (\"fact_4\", {\"text\": \"A long-term investment horizon allows investors to capture illiquidity premiums\"}),\n    (\"fact_5\", {\"text\": \"Factor investing targets specific drivers of return such as value, momentum, and quality\"}),\n    (\"fact_6\", {\"text\": \"Rebalancing portfolios periodically helps maintain target risk levels\"}),\n    (\"fact_7\", {\"text\": \"Alternative investments like reinsurance have low correlation with traditional stock and bond markets\"}),\n    (\"fact_8\", {\"text\": \"Systematic risk management frameworks help identify and mitigate portfolio vulnerabilities\"}),\n]\n\nfor key, value in investment_facts:\n    semantic_store.put(namespace, key, value)\n\nprint(f\"Stored {len(investment_facts)} investment facts in semantic memory\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search semantically - notice we don't need exact matches!\n\nqueries = [\n    \"How can I protect my portfolio from a market crash?\",\n    \"What alternative investments should I consider?\",\n    \"How should I think about risk in my portfolio?\",\n    \"What is Stone Ridge's investment approach?\",\n]\n\nfor query in queries:\n    print(f\"\\nQuery: {query}\")\n    results = semantic_store.search(namespace, query=query, limit=2)\n    for r in results:\n        print(f\"   {r.value['text']} (score: {r.score:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Task 7: Building Semantic Investment Knowledge Base\n\nLet's load the Stone Ridge 2025 Investor Letter and create a semantic knowledge base that our agent can search.\n\nThis is similar to RAG from Module 4, but now using LangGraph's Store API instead of a separate vector database."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk the investment document\nloader = PyMuPDFLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=100\n)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"Loaded and split into {len(chunks)} chunks\")\nprint(f\"\\nSample chunk:\\n{chunks[0].page_content[:200]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store chunks in semantic memory\nknowledge_namespace = (\"investment\", \"knowledge\")\n\nfor i, chunk in enumerate(chunks):\n    semantic_store.put(\n        knowledge_namespace,\n        f\"chunk_{i}\",\n        {\"text\": chunk.page_content, \"source\": \"Stone Ridge 2025 Investor Letter.pdf\"}\n    )\n\nprint(f\"Stored {len(chunks)} chunks in semantic knowledge base\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a semantic search investment chatbot\n\nclass SemanticState(TypedDict):\n    messages: Annotated[list, add_messages]\n    user_id: str\n\n\ndef semantic_investment_chatbot(state: SemanticState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"An investment chatbot that retrieves relevant facts using semantic search.\"\"\"\n    user_message = state[\"messages\"][-1].content\n    \n    # Search for relevant knowledge\n    knowledge_results = store.search(\n        (\"investment\", \"knowledge\"),\n        query=user_message,\n        limit=3\n    )\n    \n    # Build context from retrieved knowledge\n    if knowledge_results:\n        knowledge_text = \"\\n\\n\".join([f\"- {r.value['text']}\" for r in knowledge_results])\n        system_msg = f\"\"\"You are an Investment Advisory Assistant with access to the Stone Ridge investor letter knowledge base.\n\nRelevant information from your knowledge base:\n{knowledge_text}\n\nUse this information to answer the user's question. If the information doesn't directly answer their question, use your general knowledge but mention what you found.\"\"\"\n    else:\n        system_msg = \"You are an Investment Advisory Assistant. Answer investment questions helpfully.\"\n    \n    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\n# Build and compile\nbuilder3 = StateGraph(SemanticState)\nbuilder3.add_node(\"chatbot\", semantic_investment_chatbot)\nbuilder3.add_edge(START, \"chatbot\")\nbuilder3.add_edge(\"chatbot\", END)\n\nsemantic_graph = builder3.compile(\n    checkpointer=MemorySaver(),\n    store=semantic_store\n)\n\nprint(\"Semantic investment chatbot ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test semantic retrieval\nconfig = {\"configurable\": {\"thread_id\": \"semantic_thread_1\"}}\n\nquestions = [\n    \"What is Stone Ridge's view on the current market environment?\",\n    \"How does Stone Ridge approach risk management?\",\n    \"What is Stone Ridge's investment philosophy?\",\n]\n\nfor q in questions:\n    response = semantic_graph.invoke(\n        {\"messages\": [HumanMessage(content=q)], \"user_id\": \"test_user\"},\n        config\n    )\n    print(f\"\\nUser: {q}\")\n    print(f\"Assistant: {response['messages'][-1].content[:500]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Task 8: Episodic Memory (Few-Shot Learning)\n\n**Episodic memory** stores past experiences and uses them as examples for future tasks. In humans, this is like remembering the first time you provided a successful investment recommendation and using that experience to guide future interactions.\n\nFor AI agents, episodic memory is often implemented as **few-shot examples** - showing the model examples of correct behavior so it can learn patterns.\n\n### The CoALA Framework Perspective\n\n> \"Facts can be written to semantic memory, whereas **experiences** can be written to episodic memory.\"\n\nKey insight: Sometimes it's easier to **show** than **tell**. LLMs learn well from examples!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create episodic memory with past successful interactions\nepisodic_namespace = (\"agent\", \"episodes\")\n\nsuccessful_episodes = [\n    {\n        \"situation\": \"User asked about portfolio diversification with concentrated holdings\",\n        \"input\": \"I have most of my portfolio in tech stocks. How should I diversify?\",\n        \"output\": \"\"\"For a concentrated tech portfolio, I'd recommend diversifying across several dimensions:\n\n1. **Asset classes** - Add bonds, real estate (REITs), and alternatives\n2. **Geographies** - Include international developed and emerging markets\n3. **Sectors** - Shift allocation toward healthcare, consumer staples, and industrials\n4. **Alternative risk premiums** - Consider reinsurance or longevity-linked investments\n\nStart by gradually rebalancing 5-10% per quarter to avoid tax impacts. Would you like a specific rebalancing plan?\"\"\",\n        \"feedback\": \"User appreciated the structured approach and specific action items\"\n    },\n    {\n        \"situation\": \"User concerned about market downturn impact on retirement portfolio\",\n        \"input\": \"I'm 10 years from retirement and worried about a market crash. What should I do?\",\n        \"output\": \"\"\"Market timing is difficult, but there are proven strategies to protect near-retirement portfolios:\n\n**Risk reduction:**\n- Gradually shift to a more conservative allocation (e.g., 50/50 stocks/bonds)\n- Consider tail risk hedging for catastrophic downside protection\n\n**Income stability:**\n- Build a 2-3 year cash buffer for living expenses\n- Add dividend-paying stocks and investment-grade bonds\n\n**Stay invested:**\n- Keep some equity exposure for growth to combat inflation\n- Focus on quality companies with strong balance sheets\n\nWhich aspect would you like to explore in more detail?\"\"\",\n        \"feedback\": \"User found the balanced approach reassuring and actionable\"\n    },\n    {\n        \"situation\": \"User asking about alternative investments\",\n        \"input\": \"What are alternative investments and should I have them in my portfolio?\",\n        \"output\": \"\"\"Alternative investments are assets beyond traditional stocks and bonds. They can add diversification and unique return sources:\n\n**Common alternatives:**\n- Real estate (REITs, direct property)\n- Private equity and venture capital\n- Hedge funds and managed futures\n- Reinsurance and catastrophe bonds\n- Commodities and natural resources\n\n**Benefits:**\n- Low correlation with traditional markets\n- Access to different risk premiums\n- Potential for higher risk-adjusted returns\n\n**Considerations:**\n- Often less liquid than public markets\n- May require higher minimum investments\n- Due diligence is critical\n\nA typical allocation might be 10-20% of your portfolio. What's your current portfolio size and investment horizon?\"\"\",\n        \"feedback\": \"User valued the comprehensive overview without being overwhelming\"\n    },\n]\n\nfor i, episode in enumerate(successful_episodes):\n    semantic_store.put(\n        episodic_namespace,\n        f\"episode_{i}\",\n        {\n            \"text\": episode[\"situation\"],  # Used for semantic search\n            **episode\n        }\n    )\n\nprint(f\"Stored {len(successful_episodes)} episodic memories (past successful interactions)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EpisodicState(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ndef episodic_investment_chatbot(state: EpisodicState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"A chatbot that learns from past successful interactions.\"\"\"\n    user_question = state[\"messages\"][-1].content\n    \n    # Search for similar past experiences\n    similar_episodes = store.search(\n        (\"agent\", \"episodes\"),\n        query=user_question,\n        limit=1\n    )\n    \n    # Build few-shot examples from past episodes\n    if similar_episodes:\n        episode = similar_episodes[0].value\n        few_shot_example = f\"\"\"Here's an example of a similar investment question I handled well:\n\nUser asked: {episode['input']}\n\nMy response was:\n{episode['output']}\n\nThe user feedback was: {episode['feedback']}\n\nUse this as inspiration for the style, structure, and tone of your response, but tailor it to the current question.\"\"\"\n        \n        system_msg = f\"\"\"You are an Investment Advisory Assistant. Learn from your past successes:\n\n{few_shot_example}\"\"\"\n    else:\n        system_msg = \"You are an Investment Advisory Assistant. Be helpful, specific, and supportive.\"\n    \n    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\n# Build the episodic memory graph\nbuilder4 = StateGraph(EpisodicState)\nbuilder4.add_node(\"chatbot\", episodic_investment_chatbot)\nbuilder4.add_edge(START, \"chatbot\")\nbuilder4.add_edge(\"chatbot\", END)\n\nepisodic_graph = builder4.compile(\n    checkpointer=MemorySaver(),\n    store=semantic_store\n)\n\nprint(\"Episodic memory chatbot ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test episodic memory - similar question to stored episode\nconfig = {\"configurable\": {\"thread_id\": \"episodic_thread_1\"}}\n\nresponse = episodic_graph.invoke(\n    {\"messages\": [HumanMessage(content=\"I'm thinking about adding some alternative investments to my portfolio. What should I consider?\")]},\n    config\n)\n\nprint(\"User: I'm thinking about adding some alternative investments to my portfolio. What should I consider?\")\nprint(f\"\\nAssistant: {response['messages'][-1].content}\")\nprint(\"\\nNotice: The response structure mirrors the successful alternatives episode!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Procedural Memory (Self-Improving Agent)\n",
    "\n",
    "**Procedural memory** stores the rules and instructions that guide behavior. In humans, this is like knowing *how* to give good advice - it's internalized knowledge about performing tasks.\n",
    "\n",
    "For AI agents, procedural memory often means **self-modifying prompts**. The agent can:\n",
    "1. Store its current instructions in the memory store\n",
    "2. Reflect on feedback from interactions\n",
    "3. Update its own instructions to improve\n",
    "\n",
    "### The Reflection Pattern\n",
    "\n",
    "```\n",
    "User feedback: \"Your advice is too long and complicated\"\n",
    "         â†“\n",
    "Agent reflects on current instructions\n",
    "         â†“\n",
    "Agent updates instructions: \"Keep advice concise and actionable\"\n",
    "         â†“\n",
    "Future responses use updated instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize procedural memory with base instructions\nprocedural_namespace = (\"agent\", \"instructions\")\n\ninitial_instructions = \"\"\"You are an Investment Advisory Assistant.\n\nGuidelines:\n- Be objective and data-driven in your analysis\n- Provide evidence-based investment information\n- Ask clarifying questions about risk tolerance and investment goals\n- Present balanced perspectives on investment decisions\n- Always note that past performance doesn't guarantee future results\"\"\"\n\nsemantic_store.put(\n    procedural_namespace,\n    \"investment_assistant\",\n    {\"instructions\": initial_instructions, \"version\": 1}\n)\n\nprint(\"Initialized procedural memory with base instructions\")\nprint(f\"\\nCurrent Instructions (v1):\\n{initial_instructions}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ProceduralState(TypedDict):\n    messages: Annotated[list, add_messages]\n    feedback: str  # Optional feedback from user\n\n\ndef get_instructions(store: BaseStore) -> tuple[str, int]:\n    \"\"\"Retrieve current instructions from procedural memory.\"\"\"\n    item = store.get((\"agent\", \"instructions\"), \"investment_assistant\")\n    if item is None:\n        return \"You are a helpful investment advisory assistant.\", 0\n    return item.value[\"instructions\"], item.value[\"version\"]\n\n\ndef procedural_assistant_node(state: ProceduralState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"Respond using current procedural instructions.\"\"\"\n    instructions, version = get_instructions(store)\n    \n    messages = [SystemMessage(content=instructions)] + state[\"messages\"]\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\ndef reflection_node(state: ProceduralState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"Reflect on feedback and update instructions if needed.\"\"\"\n    feedback = state.get(\"feedback\", \"\")\n    \n    if not feedback:\n        return {}  # No feedback, no update needed\n    \n    # Get current instructions\n    current_instructions, version = get_instructions(store)\n    \n    # Ask the LLM to reflect and improve instructions\n    reflection_prompt = f\"\"\"You are improving an investment advisory assistant's instructions based on user feedback.\n\nCurrent Instructions:\n{current_instructions}\n\nUser Feedback:\n{feedback}\n\nBased on this feedback, provide improved instructions. Keep the same general format but incorporate the feedback.\nOnly output the new instructions, nothing else.\"\"\"\n    \n    response = llm.invoke([HumanMessage(content=reflection_prompt)])\n    new_instructions = response.content\n    \n    # Update procedural memory with new instructions\n    store.put(\n        (\"agent\", \"instructions\"),\n        \"investment_assistant\",\n        {\"instructions\": new_instructions, \"version\": version + 1}\n    )\n    \n    print(f\"\\nInstructions updated to version {version + 1}\")\n    return {}\n\n\ndef should_reflect(state: ProceduralState) -> str:\n    \"\"\"Decide whether to reflect on feedback.\"\"\"\n    if state.get(\"feedback\"):\n        return \"reflect\"\n    return \"end\"\n\n\n# Build the procedural memory graph\nbuilder5 = StateGraph(ProceduralState)\nbuilder5.add_node(\"assistant\", procedural_assistant_node)\nbuilder5.add_node(\"reflect\", reflection_node)\n\nbuilder5.add_edge(START, \"assistant\")\nbuilder5.add_conditional_edges(\"assistant\", should_reflect, {\"reflect\": \"reflect\", \"end\": END})\nbuilder5.add_edge(\"reflect\", END)\n\nprocedural_graph = builder5.compile(\n    checkpointer=MemorySaver(),\n    store=semantic_store\n)\n\nprint(\"Procedural memory graph ready (with self-improvement capability)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with initial instructions\nconfig = {\"configurable\": {\"thread_id\": \"procedural_thread_1\"}}\n\nresponse = procedural_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"How should I think about portfolio risk?\")],\n        \"feedback\": \"\"  # No feedback yet\n    },\n    config\n)\n\nprint(\"User: How should I think about portfolio risk?\")\nprint(f\"\\nAssistant (v1 instructions):\\n{response['messages'][-1].content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now provide feedback - the agent will update its own instructions!\nresponse = procedural_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"How should I think about portfolio risk?\")],\n        \"feedback\": \"Your responses are too long. Please be more concise and give me 3 actionable insights maximum.\"\n    },\n    {\"configurable\": {\"thread_id\": \"procedural_thread_2\"}}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the updated instructions\nnew_instructions, version = get_instructions(semantic_store)\nprint(f\"Updated Instructions (v{version}):\\n\")\nprint(new_instructions)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with updated instructions - should be more concise now!\nresponse = procedural_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What investment opportunities should I consider in the current market?\")],\n        \"feedback\": \"\"  # No feedback this time\n    },\n    {\"configurable\": {\"thread_id\": \"procedural_thread_3\"}}\n)\n\nprint(f\"User: What investment opportunities should I consider in the current market?\")\nprint(f\"\\nAssistant (v{version} instructions - after feedback):\")\nprint(response['messages'][-1].content)\nprint(\"\\nNotice: The response should now be more concise based on the feedback!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Task 10: Unified Investment Memory Agent\n\nNow let's combine **all 5 memory types** into a unified investment advisory agent:\n\n1. **Short-term**: Remembers current conversation (checkpointer)\n2. **Long-term**: Stores user profile across sessions (store + namespace)\n3. **Semantic**: Retrieves relevant investment knowledge (store + embeddings)\n4. **Episodic**: Uses past successful interactions as examples (store + search)\n5. **Procedural**: Adapts behavior based on feedback (store + reflection)\n\n### Memory Retrieval Flow\n\n```\nUser Query: \"What investment strategy suits my risk profile?\"\n              â”‚\n              â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  1. PROCEDURAL: Get current instructions         â”‚\nâ”‚  2. LONG-TERM: Load user profile (constraints)   â”‚\nâ”‚  3. SEMANTIC: Search investment knowledge        â”‚\nâ”‚  4. EPISODIC: Find similar past interactions     â”‚\nâ”‚  5. SHORT-TERM: Include conversation history     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â”‚\n              â–¼\n        Generate personalized, informed response\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class UnifiedState(TypedDict):\n    messages: Annotated[list, add_messages]\n    user_id: str\n    feedback: str\n\n\ndef unified_investment_assistant(state: UnifiedState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"An assistant that uses all five memory types.\"\"\"\n    user_id = state[\"user_id\"]\n    user_message = state[\"messages\"][-1].content\n    \n    # 1. PROCEDURAL: Get current instructions\n    instructions_item = store.get((\"agent\", \"instructions\"), \"investment_assistant\")\n    base_instructions = instructions_item.value[\"instructions\"] if instructions_item else \"You are a helpful investment advisory assistant.\"\n    \n    # 2. LONG-TERM: Get user profile\n    profile_items = list(store.search((user_id, \"profile\")))\n    pref_items = list(store.search((user_id, \"preferences\")))\n    profile_text = \"\\n\".join([f\"- {p.key}: {p.value}\" for p in profile_items]) if profile_items else \"No profile stored.\"\n    \n    # 3. SEMANTIC: Search for relevant knowledge\n    relevant_knowledge = store.search((\"investment\", \"knowledge\"), query=user_message, limit=2)\n    knowledge_text = \"\\n\".join([f\"- {r.value['text'][:200]}...\" for r in relevant_knowledge]) if relevant_knowledge else \"No specific knowledge found.\"\n    \n    # 4. EPISODIC: Find similar past interactions\n    similar_episodes = store.search((\"agent\", \"episodes\"), query=user_message, limit=1)\n    if similar_episodes:\n        ep = similar_episodes[0].value\n        episode_text = f\"Similar past interaction:\\nUser: {ep.get('input', 'N/A')}\\nResponse style: {ep.get('feedback', 'N/A')}\"\n    else:\n        episode_text = \"No similar past interactions found.\"\n    \n    # Build comprehensive system message\n    system_message = f\"\"\"{base_instructions}\n\n=== USER PROFILE ===\n{profile_text}\n\n=== RELEVANT INVESTMENT KNOWLEDGE ===\n{knowledge_text}\n\n=== LEARNING FROM EXPERIENCE ===\n{episode_text}\n\nUse all of this context to provide the best possible personalized response.\"\"\"\n    \n    # 5. SHORT-TERM: Full conversation history is automatically managed by the checkpointer\n    # Use summarization for long conversations\n    trimmed_messages = summarize_conversation(state[\"messages\"], max_messages=6)\n    \n    messages = [SystemMessage(content=system_message)] + trimmed_messages\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n\ndef unified_feedback_node(state: UnifiedState, config: RunnableConfig, *, store: BaseStore):\n    \"\"\"Update procedural memory based on feedback.\"\"\"\n    feedback = state.get(\"feedback\", \"\")\n    if not feedback:\n        return {}\n    \n    item = store.get((\"agent\", \"instructions\"), \"investment_assistant\")\n    if item is None:\n        return {}\n    \n    current = item.value\n    reflection_prompt = f\"\"\"Update these instructions based on feedback:\n\nCurrent: {current['instructions']}\nFeedback: {feedback}\n\nOutput only the updated instructions.\"\"\"\n    \n    response = llm.invoke([HumanMessage(content=reflection_prompt)])\n    store.put(\n        (\"agent\", \"instructions\"),\n        \"investment_assistant\",\n        {\"instructions\": response.content, \"version\": current[\"version\"] + 1}\n    )\n    print(f\"Procedural memory updated to v{current['version'] + 1}\")\n    return {}\n\n\ndef unified_route(state: UnifiedState) -> str:\n    return \"feedback\" if state.get(\"feedback\") else \"end\"\n\n\n# Build the unified graph\nunified_builder = StateGraph(UnifiedState)\nunified_builder.add_node(\"assistant\", unified_investment_assistant)\nunified_builder.add_node(\"feedback\", unified_feedback_node)\n\nunified_builder.add_edge(START, \"assistant\")\nunified_builder.add_conditional_edges(\"assistant\", unified_route, {\"feedback\": \"feedback\", \"end\": END})\nunified_builder.add_edge(\"feedback\", END)\n\n# Compile with both checkpointer (short-term) and store (all other memory types)\nunified_graph = unified_builder.compile(\n    checkpointer=MemorySaver(),\n    store=semantic_store\n)\n\nprint(\"Unified investment assistant ready with all 5 memory types!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the unified assistant\nconfig = {\"configurable\": {\"thread_id\": \"unified_thread_1\"}}\n\n# First interaction - should use semantic + long-term + episodic memory\nresponse = unified_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What investment strategy would you recommend given my profile?\")],\n        \"user_id\": \"user_alex\",  # Alex has moderate risk tolerance and ESG preferences!\n        \"feedback\": \"\"\n    },\n    config\n)\n\nprint(\"User: What investment strategy would you recommend given my profile?\")\nprint(f\"\\nAssistant: {response['messages'][-1].content}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"Memory types used:\")\nprint(\"  Long-term: Knows Alex's risk tolerance, portfolio, and ESG preferences\")\nprint(\"  Semantic: Retrieved investment knowledge from Stone Ridge letter\")\nprint(\"  Episodic: May use similar advisory episode as reference\")\nprint(\"  Procedural: Following current instructions\")\nprint(\"  Short-term: Will remember this in follow-up questions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Follow-up question (tests short-term memory)\nresponse = unified_graph.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"Can you tell me more about the alternative investments you mentioned?\")],\n        \"user_id\": \"user_alex\",\n        \"feedback\": \"\"\n    },\n    config  # Same thread\n)\n\nprint(\"User: Can you tell me more about the alternative investments you mentioned?\")\nprint(f\"\\nAssistant: {response['messages'][-1].content}\")\nprint(\"\\nNotice: The agent remembers the context from the previous message!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## â“ Question #3:\n\nHow would you decide what constitutes a **\"successful\" investment advisory interaction** worth storing as an episode? What metadata should you store alongside the episode?\n\nConsider:\n- Explicit feedback (thumbs up) vs implicit signals\n- User engagement (did they ask follow-up questions?)\n- Objective outcomes vs subjective satisfaction\n- Privacy implications of storing interaction data\n\n##### Answer:\n*Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## â“ Question #4:\n\nFor a **production investment advisory assistant**, which memory types need persistent storage (PostgreSQL) vs in-memory? How would you handle memory across multiple agent instances (e.g., Market Outlook Agent, Strategy Agent, Risk Management Agent)?\n\nConsider:\n- Which memories are user-specific vs shared?\n- Consistency requirements across agents\n- Memory expiration and cleanup policies\n- Namespace strategy for multi-agent systems\n\n##### Answer:\n*Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ—ï¸ Activity #2: Investment Memory Dashboard\n\nBuild an investment tracking system that:\n1. Tracks investment metrics over time (portfolio value, risk score, allocation drift)\n2. Uses semantic memory to find relevant investment advice\n3. Uses episodic memory to recall what advisory approaches worked before\n4. Uses procedural memory to adapt advice style\n5. Provides a synthesized \"investment summary\"\n\n### Requirements:\n- Store at least 3 investment metrics per user\n- Track metrics over multiple \"days\" (simulated)\n- Agent should reference historical data in responses\n- Generate a personalized investment summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### YOUR CODE HERE ###\n\n# Step 1: Define investment metrics schema and storage functions\ndef log_investment_metric(store, user_id: str, date: str, metric_type: str, value: float, notes: str = \"\"):\n    \"\"\"Log an investment metric for a user.\"\"\"\n    pass\n\n\ndef get_investment_history(store, user_id: str, metric_type: str = None, days: int = 7) -> list:\n    \"\"\"Get investment history for a user.\"\"\"\n    pass\n\n\n# Step 2: Create sample investment data for a user (simulate a week)\n\n\n# Step 3: Build an investment dashboard agent that:\n#   - Retrieves user's investment history\n#   - Searches for relevant advice based on patterns\n#   - Uses episodic memory for what worked before\n#   - Generates a personalized summary\n\n\n# Step 4: Test the dashboard\n# Example: \"Give me a summary of my portfolio performance this week\"\n# Example: \"My portfolio has been volatile lately. What might help?\"\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nIn this module, we explored the **5 memory types** from the CoALA framework:\n\n| Memory Type | LangGraph Component | Scope | Investment Use Case |\n|-------------|---------------------|-------|-------------------|\n| **Short-term** | `MemorySaver` + `thread_id` | Within thread | Current consultation |\n| **Long-term** | `InMemoryStore` + namespaces | Across threads | User profile, goals, constraints |\n| **Semantic** | Store + embeddings + `search()` | Across threads | Investment knowledge retrieval |\n| **Episodic** | Store + few-shot examples | Across threads | Past successful interactions |\n| **Procedural** | Store + self-reflection | Across threads | Self-improving instructions |\n\n### Key Takeaways:\n\n1. **Memory transforms chatbots into advisors** - Persistence enables personalization\n2. **Different memory types serve different purposes** - Choose based on your use case\n3. **Context management is critical** - Trim and summarize to stay within limits\n4. **Episodic memory enables learning** - Show, don't just tell\n5. **Procedural memory enables adaptation** - Agents can improve themselves\n\n### Production Considerations:\n\n- Use `PostgresSaver` instead of `MemorySaver` for persistent checkpoints\n- Use `PostgresStore` instead of `InMemoryStore` for persistent long-term memory\n- Consider TTL (Time-to-Live) policies for automatic memory cleanup\n- Implement proper access controls for user data\n- Ensure compliance with financial regulations for investment advisory data\n\n### Further Reading:\n\n- [LangGraph Memory Documentation](https://langchain-ai.github.io/langgraph/concepts/memory/)\n- [CoALA Paper](https://arxiv.org/abs/2309.02427) - Cognitive Architectures for Language Agents\n- [LangGraph Platform](https://docs.langchain.com/langgraph-platform/) - Managed infrastructure for production"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}